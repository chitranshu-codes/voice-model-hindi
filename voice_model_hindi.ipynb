{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7JTI4sDPV4y"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# INSTALL LIBRARIES\n",
        "# ==========================================\n",
        "import os\n",
        "try:\n",
        "    import faster_whisper\n",
        "    import piper_phonemize\n",
        "except ImportError:\n",
        "    print(\"‚è≥ Installing Libraries...\")\n",
        "    # 1. Install System Dependencies (Required for Piper)\n",
        "    !sudo apt-get update -y > /dev/null\n",
        "    !sudo apt-get install -y espeak-ng > /dev/null\n",
        "\n",
        "    # 2. Install Python Libraries\n",
        "    !pip install -q faster-whisper accelerate bitsandbytes gradio\n",
        "    !pip install -q piper-tts\n",
        "    !pip install -q git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "    # 3. Download Hindi Voice (Piper ONNX)\n",
        "    print(\"‚è≥ Downloading Hindi Voice Model...\")\n",
        "    !wget -q -O hindi_voice.onnx https://huggingface.co/rhasspy/piper-voices/resolve/main/hi/hi_IN/pratham/medium/hi_IN-pratham-medium.onnx\n",
        "    !wget -q -O hindi_voice.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/main/hi/hi_IN/pratham/medium/hi_IN-pratham-medium.onnx.json\n",
        "\n",
        "print(\"‚úÖ Installation Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# LOADING AI MODELS\n",
        "# ==========================================\n",
        "import torch\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "import time\n",
        "from faster_whisper import WhisperModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "\n",
        "    LLM_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    WHISPER_SIZE = \"medium\"\n",
        "    PIPER_VOICE = \"hindi_voice.onnx\"\n",
        "    OUTPUT_AUDIO = \"reply.wav\"\n",
        "\n",
        "print(\"‚è≥ Loading Models\")\n",
        "\n",
        "# 1. Load Brain (Qwen 1.5B in 4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(Config.LLM_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    Config.LLM_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 2. Load Ears (Whisper Medium)\n",
        "ear_model = WhisperModel(Config.WHISPER_SIZE, device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "print(\"‚úÖ SYSTEM READY\")"
      ],
      "metadata": {
        "id": "YuCvWBkGPYGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# THE Hindi BOT V2\n",
        "# ==========================================\n",
        "class TurboHindiBot:\n",
        "\n",
        "    def listen(self, audio_path):\n",
        "        if not audio_path: return \"\"\n",
        "\n",
        "        # Changed beam_size to 1\n",
        "        segments, _ = ear_model.transcribe(\n",
        "            audio_path,\n",
        "            language=\"hi\",\n",
        "            beam_size=1,\n",
        "            initial_prompt=\"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\", # Force Hindi\n",
        "            condition_on_previous_text=False\n",
        "        )\n",
        "        return \" \".join([s.text for s in segments]).strip()\n",
        "\n",
        "    def think(self, user_text):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a Hindi assistant. Reply ONLY in Hindi. Keep answers under 20 words.\"},\n",
        "            {\"role\": \"user\", \"content\": user_text}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=60,\n",
        "            temperature=0.3,\n",
        "            do_sample=True,\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(generated_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
        "        return response.strip()\n",
        "\n",
        "    def speak(self, text):\n",
        "        if not text: return None\n",
        "\n",
        "        # -- CLEAN THE TEXT To prevent PiperTTS crashe---\n",
        "\n",
        "        import re\n",
        "        # Remove English letters\n",
        "        clean_text = re.sub(r'[a-zA-Z]', '', text)\n",
        "        # Remove special symbols\n",
        "        clean_text = re.sub(r'[^\\w\\s\\u0900-\\u097F\\u002E\\u002C\\u003F\\u0021]', '', clean_text)\n",
        "        # Remove newlines\n",
        "        clean_text = clean_text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "        if not clean_text:\n",
        "            print(\"‚ö†Ô∏è Warning: Text was empty after cleaning.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"üó£Ô∏è Sending to Piper: {clean_text}\")\n",
        "\n",
        "\n",
        "        command = f'echo \"{clean_text}\" | piper --model {Config.PIPER_VOICE} --output_file {Config.OUTPUT_AUDIO}'\n",
        "\n",
        "        # Run with error capturing\n",
        "        try:\n",
        "            result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå PIPER ERROR: {e.stderr}\")\n",
        "            return None\n",
        "\n",
        "        if os.path.exists(Config.OUTPUT_AUDIO):\n",
        "            return Config.OUTPUT_AUDIO\n",
        "        else:\n",
        "            print(\"‚ùå Error: Audio file was not created.\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "s6yhd0InPaHb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# THE UI\n",
        "# ==========================================\n",
        "bot = TurboHindiBot()\n",
        "\n",
        "def run_chat(audio_path):\n",
        "    if audio_path is None: return None, None, None\n",
        "\n",
        "    # Pipeline\n",
        "    user_text = bot.listen(audio_path)\n",
        "    if not user_text: return None, None, None\n",
        "\n",
        "    ai_reply = bot.think(user_text)\n",
        "    audio_file = bot.speak(ai_reply)\n",
        "\n",
        "    return user_text, ai_reply, audio_file\n",
        "\n",
        "with gr.Blocks(title=\"Hindi AI Bot V2\") as demo:\n",
        "    gr.Markdown(\"#  Hindi AI Bot\")\n",
        "    gr.Markdown(\"Using **Qwen 1.5B** + **Whisper Medium** + **Piper TTS**\")\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        audio_in = gr.Audio(\n",
        "            sources=[\"microphone\", \"upload\"],\n",
        "            type=\"filepath\",\n",
        "            label=\"üé§ Record or üìÇ Upload Audio\"\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        user_out = gr.Textbox(label=\"You\")\n",
        "        ai_out = gr.Textbox(label=\"AI\")\n",
        "        audio_out = gr.Audio(label=\"Reply\", autoplay=True)\n",
        "\n",
        "    # Trigger on recording stop (Mic)\n",
        "    audio_in.stop_recording(run_chat, inputs=[audio_in], outputs=[user_out, ai_out, audio_out])\n",
        "\n",
        "    # Trigger on file upload (File)\n",
        "    audio_in.upload(run_chat, inputs=[audio_in], outputs=[user_out, ai_out, audio_out])\n",
        "\n",
        "print(\"‚è≥ Launching UI...\")\n",
        "demo.queue().launch(share=True, debug=True)\n",
        "\n",
        "# Keep Alive\n",
        "import time\n",
        "while True:\n",
        "    time.sleep(60)\n",
        "    print(\".\", end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "BcZ4xpzDPb9_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}